{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "abc29a24-5a8c-4606-a159-6b33eee87b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "df_credb = pd.read_excel('createDebate.xlsx')\n",
    "df_credb = df_credb.query(\"stance == 'prefers strict gun control' | stance == 'opposes strict gun control'\")\n",
    "df_credb = df_credb[[\"text\", \"stance\"]]\n",
    "#Encoding the stance, which is our target class\n",
    "import numpy as np\n",
    "\n",
    "if type(df_credb.stance.iloc[1]) is str:\n",
    "    class_mapping = {label: idx for idx, label in \n",
    "                    enumerate(np.unique(df_credb['stance']))}\n",
    "#Prevents accidentally overwriting the \n",
    "if type(df_credb.stance.iloc[1]) is str:\n",
    "    df_credb['stance'] = df_credb['stance'].map(class_mapping)\n",
    "    \n",
    "oppose = df_credb.query(\"stance == 0\")\n",
    "support = df_credb.query(\"stance == 1\")\n",
    "\n",
    "oppose = oppose[:100]\n",
    "support = support[:100]\n",
    "\n",
    "df_reduced = pd.concat([support,oppose])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e4fd8eb1-c95a-4e82-b215-e14a51a00941",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the BERT model and tokenizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import keras\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "\n",
    "if 'bertModel' not in locals() or 'tokenizer' not in locals():\n",
    "    #BERT base model\n",
    "    bertModel = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    #BERT Tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "820c9ec6-3176-4431-8c68-719ea81705dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some example data\n",
    "texts = df_credb.text.values.tolist()\n",
    "labels = df_credb.stance.values.tolist()\n",
    "\n",
    "#tokenize the texts\n",
    "tokens = tokenizer(texts, padding=True, truncation=True, return_tensors='np') #Use Tensorflow for faster training\n",
    "\n",
    "#Create dict of tokenized data\n",
    "tokenized_data = dict(tokens)\n",
    "\n",
    "#Encode labels as a numpy array\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "50309eed-3ff1-4356-ab79-b304ec6b9daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input layer for the model\n",
    "input_ids = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "bert_layer = bertModel({'input_ids': input_ids, 'attention_mask': attention_mask})[0]\n",
    "\n",
    "# Add a GlobalMaxPooling1D layer to reduce the sequence length dimension to 1\n",
    "pooled_output = tf.keras.layers.GlobalMaxPooling1D()(bert_layer)\n",
    "\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(1,activation='sigmoid')(pooled_output)\n",
    "\n",
    "'''dense_layer_2 = tf.keras.layers.Dense(256,activation='relu')(dense_layer_1)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(2,activation='sigmoid')(dense_layer_2)'''\n",
    "\n",
    "for layer in model.layers[:3]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer=Adam(3e-5),\n",
    "                 loss = 'mean_squared_error',\n",
    "                 metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "#model.fit(tokenized_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "424abe1a-2055-4e96-9a9c-452ba33a1ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " attention_mask (InputLayer)    [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_ids (InputLayer)         [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['attention_mask[0][0]',         \n",
      "                                thPoolingAndCrossAt               'input_ids[0][0]']              \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 512,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling1d_12 (Globa  (None, 768)         0           ['tf_bert_model[26][0]']         \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " dense_48 (Dense)               (None, 1)            769         ['global_max_pooling1d_12[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,483,009\n",
      "Trainable params: 769\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4c173226-4b60-4d98-bb3e-2654098cb1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b7cccaf0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b7cccaf0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      " 68/382 [====>.........................] - ETA: 35:05 - loss: 0.2874 - accuracy: 0.4982"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m   1641\u001b[0m     \u001b[38;5;124;03m\"\"\"The logic for one evaluation step.\u001b[39;00m\n\u001b[1;32m   1642\u001b[0m \n\u001b[1;32m   1643\u001b[0m \u001b[38;5;124;03m    This method can be overridden to support custom evaluation logic.\u001b[39;00m\n\u001b[1;32m   1644\u001b[0m \u001b[38;5;124;03m    This method is called by `Model.make_test_function`.\u001b[39;00m\n\u001b[1;32m   1645\u001b[0m \n\u001b[1;32m   1646\u001b[0m \u001b[38;5;124;03m    This function should contain the mathematical logic for one step of\u001b[39;00m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;124;03m    evaluation.\u001b[39;00m\n\u001b[1;32m   1648\u001b[0m \u001b[38;5;124;03m    This typically includes the forward pass, loss calculation, and metrics\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m \u001b[38;5;124;03m    updates.\u001b[39;00m\n\u001b[0;32m-> 1650\u001b[0m \n\u001b[1;32m   1651\u001b[0m \u001b[38;5;124;03m    Configuration details for *how* this logic is run (e.g. `tf.function`\u001b[39;00m\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;124;03m    and `tf.distribute.Strategy` settings), should be left to\u001b[39;00m\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;124;03m    `Model.make_test_function`, which can also be overridden.\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m \n\u001b[1;32m   1655\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;124;03m      data: A nested structure of `Tensor`s.\u001b[39;00m\n\u001b[1;32m   1657\u001b[0m \n\u001b[1;32m   1658\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;124;03m      A `dict` containing values that will be passed to\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;124;03m      `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\u001b[39;00m\n\u001b[1;32m   1661\u001b[0m \u001b[38;5;124;03m      values of the `Model`'s metrics are returned.\u001b[39;00m\n\u001b[1;32m   1662\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1663\u001b[0m     x, y, sample_weight \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39munpack_x_y_sample_weight(data)\n\u001b[1;32m   1665\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m device_name \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39mdevice_name\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m     54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(tokenized_data, labels, batch_size=16, epochs=2,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212bbe1f-d206-4a19-b4a3-efb0d519c0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
